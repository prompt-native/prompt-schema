# Prompt Schema V0.2

Note: Optional fields will be marked as `field?`,
otherwise it's required.

Each prompt should contain the following meta data:

```yaml
version: 0.1
type: completion | chat
vendor: <string>
model:
  name: <string>
  version?: <string>
<completion or chat content>
parameters?:
  - name: <string>
    value: <string | number | bool>
```

- `version`: The schema version, currently it's set to 0.1
- `type`: Prompt type, could be completion/chat
- `vendor`: The LLM provider,eg. google
- `model`:
  - `name`: The model that used in this prompt, eg. text-bison.
  - `version`: The model version if necessary. For example, if we want to use text-bison@001, then version could be specified as `001`

## Parameters

You can also specify LLM parameters in prompts.
Parameters are vendor specific,
and are explained by the runtime.
Parameter name should use snake case naming convensation.

```yaml
parameters?:
  - name: <string>
    value: <string | number | bool>
```

- `name`: Name of the parameter, eg. `max_tokens` or `maxOutputTokens`
- `value`: Value of this parameter, could be number, or a boolean, or just a string.

If the vendor provides complex parameter with nested structure,
then the parameter must be flatten into Key -Value pairs.

## Completion

When `type` is `completion`, then the prompt is defined as a completion mode. In general, completion mode contains a single text block of prompt, and it's a one-time exection. In contrast, you can have conversation with LLM in `chat` mode.

If a prompt is completion mode, then it must contain a prompt field, which colud be a plain string:

```yaml
prompt: <string>
```

or as structured:

```yaml
prompt:
  context?: <string>
  examples:
    - field: <string>
      values: <[string]>
  test: <[string]>
```

- `prompt`: The prompt text, in simple cases, the prompt could be a question or a instruction.
- `examples`: (Optional) A set of eaxamples that help to structure the prompt.
  Each example is structured like:

  - `field`: The field name, eg. "input", "Some input"
  - `values`: Example values of this field

- `test`: (Optional) The value that will be used as input of this field. If it's expected to be generated by LLM, then leave it empty.

  For example, The prompt could be organized like this:

  ```
  Q: <Question1>?
  A: <Answer1>
  Q: <Question2>?
  A: <Answer2>
  Q: <Question3>?
  A: <Answer3>
  Q: <Question4>?
  A:
  ```

  The equivalent yaml `examples` is:

  ```yaml
  prompt:
    examples:
      - field: Q
      values:
          - <Question1>?
          - <Question2>?
          - <Question3>?
      - field: A
      values:
          - <Answer1>
          - <Answer2>
          - <Answer3>
  test:
    - <Question4>?
  ```

## Chat

Chat mode is usually defined for conversation, and could be multi-turned.
In chat mode, prompts could contains the following fields:

```yaml
context?: <string>
examples?:
  - input: <string>
    output: <string>
history?:
  - input: <string>
    output: <string>
question: <string>
```

- `context`: (Optional) Set the background/persona of this conversation.
- `examples`: (Optional) Specify example messages of the conversation. All `input` and `output` in examples must NOT be empty.
- `history`:
  - `input`: The question asked by user
  - `output`: The output of AI. If it's the last message, means that it's expected to be answered by LLM, so the last output should be empty.
- `question`: The question that should be answered
