# Prompt Schema V0.1

Note: Optional fields will be marked as `field?`,
otherwise it's required.

Each prompt should contain the following meta data:

```yaml
version: 0.1
type: completion | chat
vendor: <string>
model: <string>
```

- `version`: The schema version, currently it's set to 0.1
- `type`: Prompt type, could be completion/chat
- `vendor`: The LLM provider,eg. google
- `model`: The model that used in this prompt, eg. text-bison

## Completion

When `type` is `completion`, then the prompt is defined as a completion mode. In general, completion mode contains a single text block of prompt, and it's a one-time exection. In contrast, you can have conversation with LLM in `chat` mode.

If a prompt is completion mode, then it could have additional fields:

```yaml
prompt: <string>
examples?:
  - name: <string>
    values: <[string]>
    test?: <string>
```

- `prompt`: The prompt text, in simple cases, the prompt could be a question or a instruction.
- `examples`: (Optional) A set of eaxamples that help to structure the prompt.
  Each example is structured like:

  - `name`: The field name, eg. "input", "Some input"
  - `values`: Example values of this field
  - `test`: (Optional) The value that will be used as input of this field. If it's expected to be generated by LLM, then leave it empty.

  For example, The prompt could be organized like this:

  ```
  Q: <Question1>?
  A: <Answer1>
  Q: <Question2>?
  A: <Answer2>
  Q: <Question3>?
  A: <Answer3>
  Q: <Question4>?
  A:
  ```

  The equivalent yaml `examples` is:

  ```yaml
  examples:
    - name: Q
      values:
        - <Question1>?
        - <Question2>?
        - <Question3>?
      test: <Question4>?
    - name: A
      values:
        - <Answer1>
        - <Answer2>
        - <Answer3>
  ```

You should follow the conventions when use `prompt` and `examples`:

- If there's no `examples`, then `prompt` should contain everything.
- If there's `examples`, then it means that this prompt is a structured prompt, `prompt` should only contain the context, and put examples into `examples` structure.

## Chat

Chat mode is usually defined for conversation, and could be multi-turned.
In chat mode, prompts could contains the following fields:

```yaml
context?: <string>
examples?:
  - input: <string>
    output: <string>
messages:
  - input: <string>
    output?: <string>
```

- `context`: (Optional) Set the background/persona of this conversation.
- `examples`: (Optional) Specify example messages of the conversation. All `input` and `output` in examples must NOT be empty.
- `messages`:
  - `input`: The question asked by user
  - `output`: (Optional) The output of AI. If it's the last message, means that it's expected to be answered by LLM, so the last output should be empty.
